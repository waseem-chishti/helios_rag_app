{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3828cf-42c0-4cad-b1f9-6025afd7d10a",
   "metadata": {},
   "source": [
    "## Files + Githubhub + Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12319533-4cd8-48ab-9de7-180da0bc5fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/envs/bot/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/apple/opt/anaconda3/envs/bot/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/apple/opt/anaconda3/envs/bot/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "DEBUG:chromadb.config:Starting component System\n",
      "DEBUG:chromadb.config:Starting component Posthog\n",
      "DEBUG:chromadb.config:Starting component OpenTelemetryClient\n",
      "DEBUG:chromadb.config:Starting component SqliteDB\n",
      "DEBUG:chromadb.config:Starting component QuotaEnforcer\n",
      "DEBUG:chromadb.config:Starting component LocalSegmentManager\n",
      "DEBUG:chromadb.config:Starting component SegmentAPI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:34000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): us.i.posthog.com:443\n",
      "DEBUG:urllib3.connectionpool:https://us.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Nov/2024 01:57:26] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import openpyxl  # For Excel files (.xlsx)\n",
    "import docx\n",
    "import re\n",
    "from bs4 import BeautifulSoup  # For HTML files\n",
    "from pptx import Presentation  # For PowerPoint files (.pptx)\n",
    "import xml.etree.ElementTree as ET  # For XML files\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from flask_cors import CORS\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import google.generativeai as genai\n",
    "from google.cloud import firestore, storage\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore as firebase_firestore\n",
    "import logging\n",
    "from pytube import YouTube\n",
    "import whisper\n",
    "speech_model = whisper.load_model(\"base\")\n",
    "# Load environment variables and configure generative model\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Initialize Firebase Admin with credentials and Firestore client\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "credentials_path = os.getenv('FIREBASE_CREDENTIALS')\n",
    "cred = credentials.Certificate(credentials_path)\n",
    "storage_client = storage.Client.from_service_account_json(credentials_path)\n",
    "BUCKET_NAME = os.getenv(\"BUCKET\")\n",
    "bucket_name = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "firebase_admin.initialize_app(cred)\n",
    "# Initialize Firestore and Cloud Storage clients\n",
    "firestore_client = firestore.client()\n",
    "\n",
    "# # Initialize Firestore and Cloud Storage clients\n",
    "# firestore_client = firebase_firestore.client()\n",
    "# storage_client = storage.Client.from_service_account_json('prj-inflectiv-dev-56e96d747576.json')\n",
    "# bucket_name = os.getenv(\"BUCKET\")\n",
    "# bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(persist_directory=\"chroma_db\"))\n",
    "\n",
    "# Dictionaries to store user-specific data and configurations\n",
    "user_chat_sessions = {}\n",
    "user_chat_history = {}\n",
    "user_vector_dbs = {}\n",
    "user_generation_config = {}\n",
    "model_instructions = \"\"\n",
    "\n",
    "default_generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('advance.html')\n",
    "\n",
    "# Helper function to sanitize user IDs for storage paths\n",
    "def sanitize_user_id(user_id):\n",
    "    return user_id.replace(\"@\", \"_\").replace(\".\", \"_\")\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Sanitize filenames to remove invalid characters.\"\"\"\n",
    "    return ''.join(c if c.isalnum() or c in (' ', '.', '_') else '_' for c in name)\n",
    "\n",
    "\n",
    "# File and Link Processing Functions\n",
    "import yt_dlp\n",
    "\n",
    "def download_youtube_video_yt_dlp(url, output_path=\"/tmp\"):\n",
    "    \"\"\"Download YouTube video using yt-dlp.\"\"\"\n",
    "    try:\n",
    "        ydl_opts = {\n",
    "            'outtmpl': os.path.join(output_path, '%(title)s.%(ext)s'),\n",
    "            'format': 'bestvideo+bestaudio/best',\n",
    "            'merge_output_format': 'mp4',  # Ensure the output is a single mp4 file\n",
    "            # 'ffmpeg_location': '/usr/bin/ffmpeg',  # Adjust path if necessary\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=True)\n",
    "            file_path = ydl.prepare_filename(info)\n",
    "            logging.info(f\"Downloaded video to: {file_path}\")\n",
    "            return file_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading YouTube video with yt-dlp: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "# import os\n",
    "import tempfile\n",
    "import logging\n",
    "def create_temp_dir():\n",
    "    return tempfile.mkdtemp(prefix=\"my_app_\")\n",
    "\n",
    "temp_dir = create_temp_dir()\n",
    "def download_github_repo(repo_url, output_path=None):\n",
    "    \"\"\"Download and extract a GitHub repository.\"\"\"\n",
    "    try:\n",
    "        output_path = output_path or create_temp_dir()\n",
    "        # Normalize the URL to ensure proper format\n",
    "        if repo_url.endswith(\".git\"):\n",
    "            repo_url = repo_url[:-4]\n",
    "\n",
    "        # Try fetching the repository archive for common branch names\n",
    "        for branch in [\"main\", \"master\"]:\n",
    "            zip_url = f\"{repo_url}/archive/refs/heads/{branch}.zip\"\n",
    "            response = requests.get(zip_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                # Create a temporary directory to store the downloaded ZIP\n",
    "                # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                # zip_path = os.path.join(temp_dir, \"repo.zip\")\n",
    "                # output_path = output_path or create_temp_dir()\n",
    "                zip_path = os.path.join(output_path, \"repo.zip\")\n",
    "                # Save the ZIP content to a file\n",
    "                with open(zip_path, \"wb\") as zip_file:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:  # Filter out keep-alive new chunks\n",
    "                            zip_file.write(chunk)\n",
    "\n",
    "                # Extract the ZIP file\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(output_path)\n",
    "                \n",
    "                logging.info(f\"GitHub repository extracted to {output_path}\")\n",
    "                return output_path\n",
    "\n",
    "        # If no valid branch is found\n",
    "        raise Exception(f\"Failed to fetch GitHub repo: HTTP 404 for branches ['main', 'master']\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading GitHub repo from {repo_url}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def process_uploaded_file(content, user_id):\n",
    "#     \"\"\"Process uploaded files to extract embeddings.\"\"\"\n",
    "#     try:\n",
    "#         # content = extract_text(blob)\n",
    "#         content=content\n",
    "#         chunks = [content]\n",
    "#         embeddings = embedding_model.encode(chunks, convert_to_tensor=True)\n",
    "#         save_to_user_index(user_id, embeddings, chunks)\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error processing file {content}: {str(e)}\")\n",
    "#         raise\n",
    "        \n",
    "def extract_text_from_youtube(video_path):\n",
    "    \"\"\"Transcribe audio from a YouTube video.\"\"\"\n",
    "    try:\n",
    "        result = speech_model.transcribe(video_path)\n",
    "        transcription = result['text']\n",
    "        logging.info(f\"Transcription complete for {video_path}\")\n",
    "        # print(\"\\n\")\n",
    "        # print(\"#######################################YOUTUBE transcription\")\n",
    "        # print(\"--YOUTTUBE---\",transcription)\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error transcribing YouTube video: {e}\")\n",
    "        raise\n",
    "        \n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_file():\n",
    "    \"\"\"Handle file and link uploads.\"\"\"\n",
    "    try:\n",
    "        user_id = sanitize_user_id(request.form.get(\"user_id\"))\n",
    "        if not user_id:\n",
    "            return jsonify({\"error\": \"User ID not provided\"}), 400\n",
    "\n",
    "        combined_content = []\n",
    "\n",
    "        # Process uploaded files\n",
    "        uploaded_files = request.files.getlist(\"files\")\n",
    "        if uploaded_files:\n",
    "            for file in uploaded_files:\n",
    "                try:\n",
    "                    if file and file.filename:\n",
    "                        user_dir = f\"{user_id}/files\"\n",
    "                        blob = bucket_name.blob(f\"{user_dir}/{sanitize_filename(file.filename)}\")\n",
    "                        blob.upload_from_file(file)\n",
    "                        file_text = extract_text(blob)\n",
    "                        combined_content.append(file_text)\n",
    "                        logging.info(f\"Processed file: {file.filename}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing file {file.filename}: {e}\")\n",
    "\n",
    "        # Process YouTube link\n",
    "        youtube_link = request.form.get(\"youtube_link\")\n",
    "        if youtube_link:\n",
    "            try:\n",
    "                user_dir = f\"{user_id}/youtube\"\n",
    "                file_path = download_youtube_video_yt_dlp(youtube_link)\n",
    "                blob = bucket_name.blob(f\"{user_dir}/{os.path.basename(file_path)}\")\n",
    "                blob.upload_from_filename(file_path)\n",
    "                youtube_transcription = extract_text_from_youtube(file_path)\n",
    "                combined_content.append(youtube_transcription)\n",
    "                logging.info(f\"Processed YouTube link: {youtube_link}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing YouTube link {youtube_link}: {e}\")\n",
    "\n",
    "        # Process GitHub link\n",
    "        github_link = request.form.get(\"github_link\")\n",
    "        if github_link:\n",
    "            try:\n",
    "                user_dir = f\"{user_id}/github\"\n",
    "                repo_path = download_github_repo(github_link)\n",
    "                for root, _, files in os.walk(repo_path):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        blob = bucket_name.blob(f\"{user_dir}/{file}\")\n",
    "                        blob.upload_from_filename(file_path)\n",
    "                        file_text = extract_text(blob)\n",
    "                        combined_content.append(file_text)\n",
    "                logging.info(f\"Processed GitHub repository: {github_link}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing GitHub link {github_link}: {e}\")\n",
    "\n",
    "        # Combine and process embeddings if there is content\n",
    "        if combined_content:\n",
    "            embeddings = embedding_model.encode(combined_content, convert_to_tensor=False)\n",
    "            result = save_to_user_index(user_id, embeddings, combined_content)\n",
    "            if result[\"status\"] == \"success\":\n",
    "                return jsonify({\"message\": result[\"message\"]}), 200\n",
    "            else:\n",
    "                return jsonify({\"error\": result[\"message\"]}), 500\n",
    "        else:\n",
    "            return jsonify({\"message\": \"No valid content uploaded.\"}), 400\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in upload_file: {e}\")\n",
    "        return jsonify({\"error\": f\"Exception occurred: {e}\"}), 500\n",
    "\n",
    "\n",
    "#         return jsonify({\"error\": f\"Exception occurred: {str(e)}\"}), 500\n",
    "\n",
    "\n",
    "### Function to store selected vector database\n",
    "@app.route(\"/select_vector_db\", methods=[\"POST\"])\n",
    "def select_vector_db():\n",
    "    try:\n",
    "        data = request.json\n",
    "        user_id = sanitize_user_id(data.get(\"user_id\"))\n",
    "        vector_db = data.get(\"vector_db\")\n",
    "\n",
    "        if not user_id or not vector_db:\n",
    "            return jsonify({\"error\": \"User ID and vector DB selection are required\"}), 400\n",
    "\n",
    "        firestore_client.collection(\"users\").document(user_id).set({\"vector_db\": vector_db}, merge=True)\n",
    "        user_vector_dbs[user_id] = vector_db\n",
    "        return jsonify({\"message\": f\"Vector database set to {vector_db} for user {user_id}\"}), 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in select_vector_db: {str(e)}\")\n",
    "        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n",
    "\n",
    "# Function to update generation configuration\n",
    "@app.route(\"/update_config\", methods=[\"POST\"])\n",
    "def update_config():\n",
    "    try:\n",
    "        data = request.json\n",
    "        user_id = sanitize_user_id(data.get(\"user_id\"))\n",
    "\n",
    "        if not user_id:\n",
    "            return jsonify({\"error\": \"User ID is required\"}), 400\n",
    "\n",
    "        config = user_generation_config.get(user_id, default_generation_config.copy())\n",
    "        config[\"temperature\"] = data.get(\"temperature\", config[\"temperature\"])\n",
    "        config[\"top_p\"] = data.get(\"top_p\", config[\"top_p\"])\n",
    "        config[\"top_k\"] = data.get(\"top_k\", config[\"top_k\"])\n",
    "        config[\"max_output_tokens\"] = data.get(\"max_output_tokens\", config[\"max_output_tokens\"])\n",
    "        \n",
    "        user_generation_config[user_id] = config\n",
    "        firestore_client.collection(\"users\").document(user_id).set(config, merge=True)\n",
    "        \n",
    "        return jsonify({\"message\": \"Generation configuration updated successfully\"}), 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in update_config: {str(e)}\")\n",
    "        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n",
    "\n",
    "\n",
    "@app.route(\"/set_model\", methods=[\"POST\"])\n",
    "def set_model():\n",
    "    global model\n",
    "    try:\n",
    "        data = request.json\n",
    "        user_id = data.get(\"user_id\")\n",
    "        if not user_id:\n",
    "            return jsonify({\"error\": \"User ID is required\"}), 400\n",
    "\n",
    "        selected_model = \"gemini-1.5-flash\"\n",
    "        model = genai.GenerativeModel(model_name=selected_model)\n",
    "        return jsonify({\"message\": f\"Model set to {selected_model} for user {user_id}\"}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n",
    "# Set instructions in Firestore\n",
    "@app.route(\"/set_instructions\", methods=[\"POST\"])\n",
    "def set_instructions():\n",
    "    try:\n",
    "        data = request.json\n",
    "        user_id = sanitize_user_id(data.get(\"user_id\"))\n",
    "        instructions = data.get(\"instructions\")\n",
    "\n",
    "        if not user_id or not instructions:\n",
    "            return jsonify({\"error\": \"User ID and instructions are required\"}), 400\n",
    "\n",
    "        firestore_client.collection(\"users\").document(user_id).set({\"instructions\": instructions}, merge=True)\n",
    "        return jsonify({\"message\": \"Instructions set successfully\"}), 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in set_instructions: {str(e)}\")\n",
    "        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n",
    "\n",
    "\n",
    "##############----text processing---########\n",
    "\n",
    "def remove_source_brackets(text):\n",
    "    # print(\"-----text----\",text)\n",
    "    # return cleaned_text\n",
    "    # Regex pattern to match hyperlinks starting with http or https\n",
    "    hyperlink_pattern = r\"https?://[^\\s]+\"\n",
    "    \n",
    "    # Find all hyperlinks in the text\n",
    "    hyperlinks = re.findall(hyperlink_pattern, text)\n",
    "    \n",
    "    # Placeholder for links to avoid modifying them\n",
    "    placeholder = \"<<<LINK>>>\"\n",
    "    \n",
    "    # Replace hyperlinks with a placeholder\n",
    "    for link in hyperlinks:\n",
    "        text = text.replace(link, placeholder, 1)\n",
    "    \n",
    "    # Remove all remaining # characters\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    \n",
    "    # Restore the hyperlinks back in the text\n",
    "    for link in hyperlinks:\n",
    "        text = text.replace(placeholder, link, 1)\n",
    "    cleaned_text = re.sub(r'【.*#?】', '', text)\n",
    "    cleaned_text=re.sub(r'\\s+\\.', '.', cleaned_text)\n",
    "    characters=\"*[])\"\n",
    "    for char in characters:\n",
    "        cleaned_text=cleaned_text.replace(char,'')\n",
    "    braces=\"(\"\n",
    "    for br in braces:\n",
    "        cleaned_text=cleaned_text.replace(br,' ')\n",
    "    #################Remove link duplication\n",
    "    # Regular expression to match URLs\n",
    "    url_pattern = re.compile(r'https?://[^\\s<>\"]+')\n",
    "    # Find all links in the text\n",
    "    links = url_pattern.findall(cleaned_text)\n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for link in links:\n",
    "        if link not in seen:\n",
    "            seen.add(link)\n",
    "            unique_links.append(link)\n",
    "    # Rebuild the text with unique links\n",
    "    def replace_links(match):\n",
    "        url = match.group(0)\n",
    "        if url in unique_links:\n",
    "            unique_links.remove(url)  # Ensure we replace only the first occurrence\n",
    "            return url\n",
    "        return ''  # Remove duplicates by returning an empty string\n",
    "    # Use the URL pattern to replace links in the text\n",
    "    \n",
    "    cleaned_text = url_pattern.sub(replace_links, cleaned_text)\n",
    "    cleaned_text=cleaned_text.replace(\"```javascript\", \"\").replace(\"```\", \"\").strip()\n",
    "    # print(\"-----cleaned_text----\",cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Function to extract text from uploaded files\n",
    "def extract_text(blob):\n",
    "    text = \"\"\n",
    "      # file_path = f\"/tmp/{blob.name.split('/')[-1]}\"\n",
    "    file_path = f\"/tmp/{blob.name.split('/')[-1]}\"\n",
    "    blob.download_to_filename(file_path)\n",
    "\n",
    "    # print(\"--file_path--\",file_path)\n",
    "\n",
    "    try:\n",
    "        if file_path.endswith('.pdf'):\n",
    "            with fitz.open(file_path) as pdf:\n",
    "                for page in pdf:\n",
    "                    text += page.get_text()\n",
    "        elif file_path.endswith('.docx'):\n",
    "            doc = docx.Document(file_path)\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            wb = openpyxl.load_workbook(file_path)\n",
    "            for sheet in wb.sheetnames:\n",
    "                ws = wb[sheet]\n",
    "                for row in ws.iter_rows(values_only=True):\n",
    "                    text += ' '.join([str(cell) for cell in row if cell is not None]) + \"\\n\"\n",
    "        elif file_path.endswith('.html'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                soup = BeautifulSoup(f, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "        elif file_path.endswith('.pptx'):\n",
    "            presentation = Presentation(file_path)\n",
    "            for slide in presentation.slides:\n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, \"text\"):\n",
    "                        text += shape.text + \"\\n\"\n",
    "        elif file_path.endswith('.txt') or file_path.endswith('.md'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        elif file_path.endswith('.xml'):\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            text = ' '.join(elem.text for elem in root.iter() if elem.text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Log timestamp with timezone awareness\n",
    "current_timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def save_to_user_index(user_id, embeddings, chunks):\n",
    "    \"\"\"Save embeddings and chunks as JSON in Cloud Storage and reference in Firestore.\"\"\"\n",
    "    try:\n",
    "        # Ensure embeddings are a NumPy array\n",
    "        if not isinstance(embeddings, np.ndarray):\n",
    "            embeddings = np.array(embeddings)\n",
    "\n",
    "        # Reshape embeddings if they are 1D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "\n",
    "        # Prepare data to save\n",
    "        vector_store_data = {\n",
    "            \"embeddings\": embeddings.tolist(),  # Convert to list for JSON serialization\n",
    "            \"chunks\": chunks  # Store chunks as a list\n",
    "        }\n",
    "\n",
    "        # Save to Cloud Storage\n",
    "        vector_store_json = json.dumps(vector_store_data)\n",
    "        blob = bucket_name.blob(f\"{user_id}/vector_store/vector_store.json\")\n",
    "        blob.upload_from_string(vector_store_json, content_type=\"application/json\")\n",
    "\n",
    "        # Save metadata to Firestore\n",
    "        vector_data = {\n",
    "            \"embeddings_location\": f\"{user_id}/vector_store/vector_store.json\",\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"source\": \"user_upload\"\n",
    "            }\n",
    "        }\n",
    "        firestore_client.collection(\"user_embeddings\").document(user_id).set(vector_data)\n",
    "        logging.info(f\"Embeddings and chunks saved to Cloud Storage for user_id: {user_id}\")\n",
    "\n",
    "        return {\"status\": \"success\", \"message\": \"Embeddings saved successfully.\"}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving embeddings for user_id {user_id}: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"message\": f\"Error saving embeddings: {str(e)}\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_context(user_id, query_embedding, top_k=5):\n",
    "    \"\"\"Retrieve the most relevant context chunks using cosine similarity.\"\"\"\n",
    "    try:\n",
    "        # Fetch metadata from Firestore\n",
    "        vector_data = firestore_client.collection(\"user_embeddings\").document(user_id).get().to_dict()\n",
    "        if not vector_data or \"embeddings_location\" not in vector_data:\n",
    "            logging.warning(f\"No embeddings location found for user_id: {user_id}\")\n",
    "            return \"No relevant data found.\", []\n",
    "\n",
    "        # Load embeddings and chunks from Cloud Storage\n",
    "        blob = bucket_name.blob(vector_data[\"embeddings_location\"])\n",
    "        vector_store_data = json.loads(blob.download_as_text())\n",
    "        embeddings = np.array(vector_store_data[\"embeddings\"])\n",
    "        chunks = vector_store_data[\"chunks\"]\n",
    "\n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim != 2:\n",
    "            raise ValueError(f\"Stored embeddings must be 2D arrays. Found: {embeddings.ndim}D.\")\n",
    "\n",
    "        # Validate query_embedding shape\n",
    "        query_embedding = np.array(query_embedding)\n",
    "        if query_embedding.ndim == 2:\n",
    "            query_embedding = query_embedding[0]  # Take the first vector if batched\n",
    "        elif query_embedding.ndim != 1:\n",
    "            raise ValueError(\"Query embedding must be a single vector or a 2D batch.\")\n",
    "\n",
    "        # Check for dimensional match\n",
    "        if query_embedding.shape[0] != embeddings.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"Query embedding dimensions do not match stored embeddings: \"\n",
    "                f\"Query {query_embedding.shape[0]}, Stored {embeddings.shape[1]}\"\n",
    "            )\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "\n",
    "        # Fetch the top-k relevant chunks\n",
    "        relevant_chunks = [chunks[i] for i in top_indices]\n",
    "        combined_content = \"\\n\".join(relevant_chunks)\n",
    "        ######\n",
    "        # logging.info(f\"Top {top_k} relevant chunks for user_id {user_id}: {relevant_chunks}\")\n",
    "\n",
    "        return combined_content, relevant_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving relevant context for user_id {user_id}: {str(e)}\")\n",
    "        return \"No relevant data found.\", []\n",
    "\n",
    "\n",
    "\n",
    "def log_user_query(user_id, query, relevant_text, response, embeddings):\n",
    "    \"\"\"Log user queries and responses.\"\"\"\n",
    "    try:\n",
    "        # Flatten embeddings to avoid nested array issues\n",
    "        if isinstance(embeddings, np.ndarray):\n",
    "            embeddings = embeddings.flatten().tolist()\n",
    "\n",
    "        # Prepare query log data\n",
    "        query_log_data = {\n",
    "            \"query\": query,\n",
    "            \"relevant_text\": relevant_text,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"embeddings\": embeddings  # Flattened embeddings\n",
    "        }\n",
    "\n",
    "        # Save log to Firestore\n",
    "        log_collection = firestore_client.collection(\"user_query_logs\").document(user_id)\n",
    "        log_collection.collection(\"logs\").add(query_log_data)\n",
    "\n",
    "        # Save logs to Cloud Storage as a backup\n",
    "        user_logs = {\"logs\": []}\n",
    "        log_blob = bucket_name.blob(f\"{user_id}/query_logs/query_log.json\")\n",
    "\n",
    "        if log_blob.exists():\n",
    "            user_logs = json.loads(log_blob.download_as_text())\n",
    "\n",
    "        # Append the new log entry\n",
    "        user_logs[\"logs\"].append(query_log_data)\n",
    "        log_blob.upload_from_string(json.dumps(user_logs, indent=4), content_type=\"application/json\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error logging query for user_id {user_id}: {str(e)}\")\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    \"\"\"Chat endpoint for user queries.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        user_id = sanitize_user_id(data.get(\"user_id\"))\n",
    "        message = data.get(\"message\")\n",
    "\n",
    "        if not user_id or not message:\n",
    "            return jsonify({\"error\": \"User ID and message are required\"}), 400\n",
    "\n",
    "        # Initialize user-specific chat session if it doesn't exist\n",
    "        if user_id not in user_chat_sessions:\n",
    "            user_chat_sessions[user_id] = model.start_chat(history=[])\n",
    "            user_chat_history[user_id] = []\n",
    "\n",
    "        # Retrieve user instructions from Firestore\n",
    "        user_doc = firestore_client.collection(\"users\").document(user_id).get()\n",
    "        user_data = user_doc.to_dict() if user_doc.exists else {}\n",
    "        model_instructions = user_data.get(\"instructions\", \"\")\n",
    "\n",
    "        # Encode the user query to find relevant context\n",
    "        query_embedding = embedding_model.encode([message], convert_to_tensor=False)\n",
    "        context_content, relevant_chunks = get_relevant_context(user_id, query_embedding, top_k=5)\n",
    "\n",
    "        if context_content == \"No relevant data found.\":\n",
    "            logging.warning(f\"No relevant context found for user_id: {user_id}\")\n",
    "\n",
    "        # Prepare model chat session\n",
    "        chat_session = user_chat_sessions[user_id]\n",
    "        chat_session.history.extend([\n",
    "            {\"role\": \"user\", \"parts\": [message]},\n",
    "            {\"role\": \"model\", \"parts\": [context_content + \"\\n\" + model_instructions]}\n",
    "        ])\n",
    "\n",
    "        # Get the model's response\n",
    "        response = chat_session.send_message(message)\n",
    "        bot_response = response.text.strip() if response and response.text else \"Error: No response from model\"\n",
    "        bot_response = remove_source_brackets(bot_response)\n",
    "\n",
    "        # Log the query and response\n",
    "        log_user_query(user_id, query=message, relevant_text=context_content, response=bot_response, embeddings=query_embedding)\n",
    "\n",
    "        # Store chat history\n",
    "        user_chat_history[user_id].append({\"user\": message, \"bot\": bot_response})\n",
    "        user_chat_history[user_id] = user_chat_history[user_id][-4:]  # Keep only the last 4 exchanges\n",
    "\n",
    "        return jsonify({\"response\": bot_response}), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in chat endpoint: {str(e)}\")\n",
    "        return jsonify({\"error\": f\"Error generating response: {str(e)}\"}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=34000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7904608-c870-4b9e-893f-353d1f3c1ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11970245-3fb6-43d6-a3e3-056d4e672fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1830a1-9958-47ba-a766-04c919f93ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eeddc7f-9651-4b86-a9e7-6b53f35e038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655b741-a886-44e3-b78a-e77794fb7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install yt-dlp whisper sentence-transformers flask flask-cors google-cloud-storage firebase-admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90516983-5f0e-4a52-8e8b-23c6d506ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e30cee-32dc-47b0-b56c-12654dd43f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb66ecb-e903-431a-bf17-3ef7f397f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pytube"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
